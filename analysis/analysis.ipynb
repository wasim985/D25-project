{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Initialization\n",
    "First Let's load the data and word embeddings. Note the original paper only used data from 2017 and top 10k subreddits. \\\n",
    "For sake of time we shall use data from From a downsampled data set from 2019 - 2021 which has top subreddits \\\n",
    "Additionally since some columns are mostly blank (e.g: self text in posts), we will only be using the columns that are useful  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading posts and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wasimroks\\AppData\\Local\\Temp\\ipykernel_20136\\287929339.py:7: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  posts = pd.read_csv('text_submissions.csv',skipinitialspace=True, usecols=post_fields)\n",
      "C:\\Users\\Wasimroks\\AppData\\Local\\Temp\\ipykernel_20136\\287929339.py:10: DtypeWarning: Columns (2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  comments = pd.read_csv('text_comments.csv',skipinitialspace=True, usecols=comment_fields)\n"
     ]
    }
   ],
   "source": [
    "#Import the data and filter accordingly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# #read posts data\n",
    "post_fields = ['id','author', 'created_utc','score','title','subreddit']\n",
    "posts = pd.read_csv('text_submissions.csv',skipinitialspace=True, usecols=post_fields)\n",
    "# read comment data\n",
    "comment_fields = ['author','subreddit','link_id']\n",
    "comments = pd.read_csv('text_comments.csv',skipinitialspace=True, usecols=comment_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter posts with no authors\n",
    "posts = posts[(posts['author'] != '[deleted]')]\n",
    "#parse time data\n",
    "posts['created_utc'] = pd.to_numeric(posts['created_utc'], errors='coerce')\n",
    "posts['time'] = pd.to_datetime(posts['created_utc'],utc=True,unit='s')\n",
    "#drop rows with na\n",
    "posts = posts.dropna()\n",
    "(posts['time'].dt.year).value_counts()\n",
    "\n",
    "# comments['created_utc'] = pd.to_numeric(comments['created_utc'], errors='coerce')\n",
    "# comments['time'] = pd.to_datetime(comments['created_utc'],utc=True,unit='s')\n",
    "comments = comments[comments['author'] != '[deleted]']\n",
    "comments = comments.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial GS-Score calculations and save them to a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the preexisting word2vec embeddings from social dimensions github \\\n",
    "Calculate GS-Scores as outlined in the paper and save them to a csv \\\n",
    "It is very time demanding to work with all the comment / post data, we rather just use all valid comments once to generate the gs-scores per user and save them for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "meta = pd.read_csv('embedding-metadata.tsv', sep='\\t', header=None)\n",
    "meta.columns = meta.iloc[0]\n",
    "meta = meta.reindex(meta.index.drop(0))\n",
    "meta.set_index(meta.columns[0], inplace=True)\n",
    "#note all vectors are normalized\n",
    "vectors = pd.read_csv('embedding-vectors.tsv', sep='\\t', header=None)\n",
    "vectors.set_index(meta.index, inplace=True)\n",
    "vectors = vectors.divide(np.linalg.norm(vectors.values, axis=1), axis='rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter subreddits to those which we can use with the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = posts[posts['subreddit'].isin(meta.index.to_list())]\n",
    "comments = comments[comments['subreddit'].isin(meta.index.to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30812380"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30812380\n",
      "2033348\n"
     ]
    }
   ],
   "source": [
    "print(comments.shape[0])\n",
    "print(posts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code is inspired by implementation by: https://github.com/ptuls/movielens-diversity-metric\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# center according to the paper is average of community vectors\n",
    "def compute_center(vectors, subreddits):\n",
    "    center = np.zeros(len(vectors.columns))\n",
    "    weight = 0\n",
    "    for subreddit in subreddits:\n",
    "        try:\n",
    "            subreddit_vec = vectors.loc[subreddit]\n",
    "            center += subreddit_vec\n",
    "            weight += 1\n",
    "        except KeyError:\n",
    "            print('Subreddit '+subreddit+' not in embedding')\n",
    "            continue\n",
    "    return center / weight\n",
    "\n",
    "# the score computation is sum of cosine similarities divided by number of unique communities contributed to\n",
    "def compute_score(vectors,subreddits,center):\n",
    "    score = 0\n",
    "    weight = 0\n",
    "    for subreddit in subreddits:\n",
    "        try:\n",
    "            subreddit_vec = vectors.loc[subreddit]\n",
    "            score +=  cosine_similarity(subreddit_vec, center)\n",
    "            weight += 1\n",
    "        except KeyError:\n",
    "            print('Subreddit '+subreddit+' not in embedding')\n",
    "            continue\n",
    "    return score / weight\n",
    "\n",
    "def generalist_specialist_score(vectors,subreddits):\n",
    "    #reduce our overhead\n",
    "    if(len(subreddits)<=1 or len(set(subreddits))==1):\n",
    "        return 1.0\n",
    "    center = compute_center(vectors,subreddits)\n",
    "    score = compute_score(vectors,subreddits,center)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use community embeddings to generate scores of posters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "usergroupings = comments.groupby('author')['subreddit'].apply(list).reset_index(name='subreddits_used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "usergroupings['gs_scores'] = usergroupings['subreddits_used'].apply(lambda x: generalist_specialist_score(vectors, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usergroupings['number_of_engagements']=usergroupings['subreddits_used'].str.len()\n",
    "usergroupings[['author','gs_scores','number_of_engagements']].to_csv('gs_scores_of_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usergroupings[usergroupings['number_of_engagements']==1]\n",
    "generalist_specialist_score(vectors,['AskReddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize NLTK to conduct sentiment analysis on comments / submissions \\\n",
    "This is very time consuming do this when you have more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Wasimroks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "C:\\Users\\Wasimroks\\AppData\\Local\\Temp\\ipykernel_20136\\12299670.py:6: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  post_title_sentiment = posts[\"title\"].loc[:100000].apply(lambda x: pd.Series(sia.polarity_scores(x)))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "#Test on small dataset for now\n",
    "post_title_sentiment = posts[\"title\"].loc[:100000].apply(lambda x: pd.Series(sia.polarity_scores(x)))\n",
    "posts[[\"neg\", \"neu\", \"pos\",\"compound\"]] = post_title_sentiment[[\"neg\", \"neu\", \"pos\",\"compound\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS-Score and Sentiment relationship\n",
    "Investigate and study relationship between sentiment and GS-scores \\\n",
    "Utilize Vader to study basic emotions such as positive, negative, neutral \\\n",
    "Visualize as line graph of GS-score vs each sentiment \\\n",
    "Breakdown Average community sentiment along with communities as a Grid \\\n",
    "Hypothesis to test: Can GS-scores of communities contribute to their emotions and strenght? \\\n",
    "                    Are specialists more likely to be enthusiastic, since they are more picky?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS-Score and submission statistics\n",
    "Compare submissions and comments of each user \\\n",
    "Utilize the ratio of submissions vs comments in a time frame to see if the user is a actively creating \\\n",
    "Categorize users as active creators \\\n",
    "Investigate the communities the user likes to submit in, is it similar to the ones they comment in (Ones used to make gs scores) \\\n",
    "Train a model with and without GS-scores to see if active contributors can be identified \\\n",
    "Compare Elite posters (top 5% posts) with community GS-scores, the original paper suggested elite commenters are generalists, is it also true for posters? \\\n",
    "Hypothesis to test: Can GS-Score be a good indicator of active contributer (likes to create submissions)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User GS-Score's relation Post Submission Rate and Avg Rating\n",
    "User GS-Score and Sentiment of their comments\n",
    "Community GS-Score's and Top Post submitters [Elite Posters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GS-scores of Elite Posters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use social dimensions to categorize community wide specialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create social dimenstions of each community \\\n",
    "Get community GS scores of posters \\\n",
    "Visualize embeddings via social dimensions, and cluster using kmeans \\\n",
    "find optimum number of clusters using elbow-method \\\n",
    "Are certain social clusters of a higher GS-score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
