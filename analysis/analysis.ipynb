{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Initialization and Pre-processing for caching purposes\n",
    "First Let's load the data and word embeddings. Note the original paper only used data from 2017 and top 10k subreddits. \\\n",
    "For sake of time we shall use data from From a downsampled data set from 2019 - 2021 which has top subreddits \\\n",
    "Additionally since some columns are mostly blank (e.g: self text in posts), we will only be using the columns that are useful  \\\n",
    "Pre-process the GS-scores and store them in a csv for later use, similarly pre-process sentiments for later user \\\n",
    "We do this so that we can load posts and comments without loading in text which is very memory intensive \\\n",
    "For the GS-scores, to be accurate we utilize as much data as we can and it can be very costly to recalculate them \\\n",
    "Some other files such as scores, embedding meta data, and embedding vectors are from the CSSLab github (see README) \\\n",
    "Note: most files are >50Mb and can't be included in the repo, I'll include a seperate google drive link for most of them\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading ALL posts and comments with text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wasimroks\\AppData\\Local\\Temp\\ipykernel_19188\\3742116792.py:7: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  posts = pd.read_csv('text_submissions.csv',skipinitialspace=True, usecols=post_fields)\n",
      "C:\\Users\\Wasimroks\\AppData\\Local\\Temp\\ipykernel_19188\\3742116792.py:10: DtypeWarning: Columns (2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  comments = pd.read_csv('text_comments.csv',skipinitialspace=True, usecols=comment_fields)\n"
     ]
    }
   ],
   "source": [
    "#Import the data and filter accordingly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# #read posts data\n",
    "post_fields = ['id','author', 'created_utc','score','title','subreddit']\n",
    "posts = pd.read_csv('text_submissions.csv',skipinitialspace=True, usecols=post_fields)\n",
    "# read comment data\n",
    "comment_fields = ['id','author','subreddit','link_id','body']\n",
    "comments = pd.read_csv('text_comments.csv',skipinitialspace=True, usecols=comment_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter posts with no authors\n",
    "posts = posts[(posts['author'] != '[deleted]')]\n",
    "#parse time data\n",
    "posts['created_utc'] = pd.to_numeric(posts['created_utc'], errors='coerce')\n",
    "posts['time'] = pd.to_datetime(posts['created_utc'],utc=True,unit='s')\n",
    "#drop rows with na\n",
    "posts = posts.dropna()\n",
    "(posts['time'].dt.year).value_counts()\n",
    "\n",
    "# comments['created_utc'] = pd.to_numeric(comments['created_utc'], errors='coerce')\n",
    "# comments['time'] = pd.to_datetime(comments['created_utc'],utc=True,unit='s')\n",
    "comments = comments[comments['author'] != '[deleted]']\n",
    "comments = comments.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments[comments['body'] != '[removed]']\n",
    "comments = comments[comments['body'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = posts[posts['title'] != '[removed]']\n",
    "posts = posts[posts['title'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial GS-Score calculations and save them to a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the preexisting word2vec embeddings from social dimensions github \\\n",
    "Calculate GS-Scores as outlined in the paper and save them to a csv \\\n",
    "It is very time demanding to work with all the comment / post data, we rather just use all valid comments once to generate the gs-scores per user and save them for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "meta = pd.read_csv('embedding-metadata.tsv', sep='\\t', header=None)\n",
    "meta.columns = meta.iloc[0]\n",
    "meta = meta.reindex(meta.index.drop(0))\n",
    "meta.set_index(meta.columns[0], inplace=True)\n",
    "#note all vectors are normalized\n",
    "vectors = pd.read_csv('embedding-vectors.tsv', sep='\\t', header=None)\n",
    "vectors.set_index(meta.index, inplace=True)\n",
    "vectors = vectors.divide(np.linalg.norm(vectors.values, axis=1), axis='rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter subreddits to those which we can use with the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = posts[posts['subreddit'].isin(meta.index.to_list())]\n",
    "comments = comments[comments['subreddit'].isin(meta.index.to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30811298"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30811298\n",
      "2033336\n"
     ]
    }
   ],
   "source": [
    "print(comments.shape[0])\n",
    "print(posts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code is inspired by implementation by: https://github.com/ptuls/movielens-diversity-metric\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# center according to the paper is average of community vectors\n",
    "def compute_center(vectors, subreddits):\n",
    "    center = np.zeros(len(vectors.columns))\n",
    "    weight = 0\n",
    "    for subreddit in subreddits:\n",
    "        try:\n",
    "            subreddit_vec = vectors.loc[subreddit]\n",
    "            center += subreddit_vec\n",
    "            weight += 1\n",
    "        except KeyError:\n",
    "            print('Subreddit '+subreddit+' not in embedding')\n",
    "            continue\n",
    "    return center / weight\n",
    "\n",
    "# the score computation is sum of cosine similarities divided by number of unique communities contributed to\n",
    "def compute_score(vectors,subreddits,center):\n",
    "    score = 0\n",
    "    weight = 0\n",
    "    for subreddit in subreddits:\n",
    "        try:\n",
    "            subreddit_vec = vectors.loc[subreddit]\n",
    "            score +=  cosine_similarity(subreddit_vec, center)\n",
    "            weight += 1\n",
    "        except KeyError:\n",
    "            print('Subreddit '+subreddit+' not in embedding')\n",
    "            continue\n",
    "    return score / weight\n",
    "\n",
    "def generalist_specialist_score(vectors,subreddits):\n",
    "    #reduce our overhead\n",
    "    if(len(subreddits)<=1 or len(set(subreddits))==1):\n",
    "        return 1.0\n",
    "    center = compute_center(vectors,subreddits)\n",
    "    score = compute_score(vectors,subreddits,center)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use community embeddings to generate scores of users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "usergroupings = comments.groupby('author')['subreddit'].apply(list).reset_index(name='subreddits_used')\n",
    "usergroupings['gs_scores'] = usergroupings['subreddits_used'].apply(lambda x: generalist_specialist_score(vectors, x))\n",
    "usergroupings['number_of_engagements']=usergroupings['subreddits_used'].str.len()\n",
    "usergroupings[['author','gs_scores','number_of_engagements']].to_csv('gs_scores_of_users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GS-scores using post statistics only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usergroupings = posts.groupby('author')['subreddit'].apply(list).reset_index(name='subreddits_used')\n",
    "usergroupings['gs_scores'] = usergroupings['subreddits_used'].apply(lambda x: generalist_specialist_score(vectors, x))\n",
    "usergroupings['number_of_engagements']=usergroupings['subreddits_used'].str.len()\n",
    "usergroupings[['author','gs_scores','number_of_engagements']].to_csv('gs_scores_of_creators.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize NLTK to conduct sentiment analysis on comments / submissions \\\n",
    "This is very time consuming do this when you have more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Wasimroks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wasimroks\\AppData\\Local\\Temp\\ipykernel_19188\\692299386.py:2: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  post_title_sentiment = posts[\"title\"].apply(lambda x: pd.Series(sia.polarity_scores(x)))\n"
     ]
    }
   ],
   "source": [
    "#Test on small dataset for now\n",
    "post_title_sentiment = posts[\"title\"].apply(lambda x: pd.Series(sia.polarity_scores(x)))\n",
    "post_title_sentiment['id']=posts['id']\n",
    "post_title_sentiment.to_csv('post_title_sentiments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an analysis on the comment sentiments (However only the comments that are linked to a post) \\\n",
    "This will allow us to visualize average user reception per post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comments_on_posts = comments[comments['link_id'].isin(posts['id'].to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wasimroks\\AppData\\Local\\Temp\\ipykernel_19188\\3058501220.py:2: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  post_comment_sentiment = comments_on_posts['body'].apply(lambda x: pd.Series(sia.polarity_scores(x)))\n"
     ]
    }
   ],
   "source": [
    "#Test on small dataset for now\n",
    "post_comment_sentiment = comments_on_posts['body'].apply(lambda x: pd.Series(sia.polarity_scores(x)))\n",
    "post_comment_sentiment[['id','author','link_id','subreddit']]=comments_on_posts[['id','author','link_id','subreddit']]\n",
    "post_title_sentiment.to_csv('post_comment_sentiments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and minimum required data after preprocessing\n",
    "After preprocessing all the text and GS-scores, we can use only a few relevant fields to save us time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "#load posts\n",
    "\n",
    "#load relevant comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS-Score and Sentiment relationship\n",
    "Investigate and study relationship between sentiment and GS-scores \\\n",
    "Utilize Vader to study basic emotions such as positive, negative, neutral \\\n",
    "Visualize as line graph of GS-score vs each sentiment \\\n",
    "Breakdown Average community sentiment along with communities as a Grid \\\n",
    "Hypothesis to test: Can GS-scores of communities contribute to their emotions and strenght? \\\n",
    "                    Are specialists more likely to be enthusiastic, since they are more picky?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS-Score and submission statistics\n",
    "Compare submissions and comments of each user \\\n",
    "Utilize the ratio of submissions vs comments in a time frame to see if the user is a actively creating \\\n",
    "Categorize users as active creators \\\n",
    "Investigate the communities the user likes to submit in, is it similar to the ones they comment in (Ones used to make gs scores) \\\n",
    "Train a model with and without GS-scores to see if active contributors can be identified \\\n",
    "Compare Elite posters (top 5% posts) with community GS-scores, the original paper suggested elite commenters are generalists, is it also true for posters? \\\n",
    "Hypothesis to test: Can GS-Score be a good indicator of active contributer (likes to create submissions)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User GS-Score's relation Post Submission Rate and Avg Rating\n",
    "User GS-Score and Sentiment of their comments\n",
    "Community GS-Score's and Top Post submitters [Elite Posters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GS-scores of Elite Posters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use social dimensions to categorize community wide specialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create social dimenstions of each community \\\n",
    "Get community GS scores of posters \\\n",
    "Visualize embeddings via social dimensions, and cluster using kmeans \\\n",
    "find optimum number of clusters using elbow-method \\\n",
    "Are certain social clusters of a higher GS-score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
